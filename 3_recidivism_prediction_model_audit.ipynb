{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    return re.sub(r'\\s', '', s)\n",
    "\n",
    "def are_equal(a , b):\n",
    "    return clean_str(a) == clean_str(b)\n",
    "\n",
    "def a_in_b(a , b):\n",
    "    return clean_str(a) in clean_str(b)\n",
    "\n",
    "def add_cols (df, columns, default_value = 0):\n",
    "    for k, e in enumerate(columns):\n",
    "        i = common_data([e], df.columns, are_equal)\n",
    "\n",
    "        if i == -1:\n",
    "            df[e] = default_value\n",
    "    return df\n",
    "\n",
    "def common_data(list1, list2, comparisonf): \n",
    "    result = -1\n",
    "  \n",
    "    for k, x in enumerate(list1): \n",
    "        for y in list2: \n",
    "            \n",
    "            if comparisonf(x,y):\n",
    "                result = k\n",
    "                return result  \n",
    "                  \n",
    "    return result\n",
    "\n",
    "def prepare_data(df, feat2dummie, cols_to_pow, columns, drop_first = True, max_pow = 3):\n",
    "    \n",
    "    for i in feat2dummie:\n",
    "        one_hot = pd.get_dummies(df[i], prefix=i, drop_first=drop_first)\n",
    "        df = df.drop(i,axis = 1)\n",
    "        df = df.join(one_hot)\n",
    "\n",
    "    is_recid, is_violent_recid = df['is_recid'], df['is_violent_recid']\n",
    "\n",
    "    columns_to_drop = ['is_recid', 'is_violent_recid']\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    \n",
    "    dft = df.copy()\n",
    "\n",
    "    for k,i in enumerate(cols_to_pow):\n",
    "        for j in range(1, max_pow):\n",
    "            dft[i+str(j+1)] = pow(df[i], j+1)\n",
    "    \n",
    "    dft = add_cols(dft, columns)\n",
    "    \n",
    "    return dft, is_recid, is_violent_recid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data'\n",
    "model_folder = './model'\n",
    "\n",
    "train_dataset = 'train_compas_processed.xlsx'\n",
    "validate_dataset = 'validate_compas_processed.xlsx'\n",
    "model_colums_sample = 'model_colums_sample.xlsx'\n",
    "\n",
    "model_file_name = 'rf_recidivism_prediction.sav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_init = pd.read_excel (data_folder+'/'+train_dataset)\n",
    "validate_df_init = pd.read_excel (data_folder+'/'+validate_dataset)\n",
    "\n",
    "train_df = train_df_init\n",
    "validate_df = validate_df_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.read_excel (data_folder+'/'+model_colums_sample).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8918, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pow = 3\n",
    "drop_first = True\n",
    "\n",
    "feat2dummie = ['sex','age_cat','race','c_charge_degree','c_cat','weapon_firearm']\n",
    "cols_to_pow = ['age','decile_score','priors_count','juv_count']\n",
    "\n",
    "df, is_recid, is_violent_recid = prepare_data(validate_df, feat2dummie, cols_to_pow, columns, drop_first = True, max_pow = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = pickle.load(open(model_folder+'/'+model_file_name, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bias detection in the data\n",
    "\n",
    "### Oleg O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection from the data\n",
    "\n",
    "### Oleg M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Result interpretability\n",
    "\n",
    "### Valerii & Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = rfc.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5057466708941027\n"
     ]
    }
   ],
   "source": [
    "result = roc_auc_score(is_recid, pred_result)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
